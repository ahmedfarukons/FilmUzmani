"""
RAG Pipeline - Retrieval-Augmented Generation sistemi
"""
import os

# TensorFlow'u devre dÄ±ÅŸÄ± bÄ±rak (sadece PyTorch kullan)
os.environ['USE_TF'] = 'NO'
os.environ['USE_TORCH'] = 'YES'

from typing import List, Optional
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
from langchain_ollama import ChatOllama


class RAGPipeline:
    """RAG pipeline sÄ±nÄ±fÄ± - ChromaDB ve Ã§eÅŸitli LLM'ler kullanarak soru-cevap sistemi"""
    
    def __init__(
        self, 
        persist_directory: str = "chroma_db",
        model_provider: str = "ollama",
        groq_api_key: Optional[str] = None
    ):
        """
        RAG Pipeline'Ä± baÅŸlatÄ±r
        
        Args:
            persist_directory: ChromaDB veritabanÄ± dizini
            model_provider: LLM saÄŸlayÄ±cÄ±sÄ± ("ollama" veya "groq")
            groq_api_key: Groq API anahtarÄ± (Groq kullanÄ±lacaksa)
        """
        # API anahtarlarÄ±nÄ± yÃ¼kle
        if groq_api_key is None:
            load_dotenv()
            groq_api_key = os.getenv('GROQ_API_KEY')
        
        self.groq_api_key = groq_api_key
        self.persist_directory = persist_directory
        self.model_provider = model_provider.lower()
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None
        
        # Embedding modelini baÅŸlat
        self._initialize_embeddings()
        
        # LLM'i baÅŸlat
        self._initialize_llm()
    
    def _initialize_embeddings(self):
        """Embedding modelini baÅŸlatÄ±r - Custom Transformers Embeddings (sentence-transformers olmadan)"""
        try:
            # Direkt transformers kullan, sentence-transformers'a gerek yok
            from transformers import AutoTokenizer, AutoModel
            import torch
            from langchain.embeddings.base import Embeddings
            from typing import List
            
            print("ğŸ“¥ Embedding modeli yÃ¼kleniyor (lokal, transformers kullanarak)...")
            
            # Custom embedding class
            class CustomTransformerEmbeddings(Embeddings):
                def __init__(self):
                    self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
                    self.model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
                    self.model.eval()
                
                def embed_documents(self, texts: List[str]) -> List[List[float]]:
                    embeddings = []
                    for text in texts:
                        encoded = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)
                        with torch.no_grad():
                            output = self.model(**encoded)
                        # Mean pooling
                        embedding = output.last_hidden_state.mean(dim=1).squeeze().tolist()
                        embeddings.append(embedding)
                    return embeddings
                
                def embed_query(self, text: str) -> List[float]:
                    encoded = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)
                    with torch.no_grad():
                        output = self.model(**encoded)
                    embedding = output.last_hidden_state.mean(dim=1).squeeze().tolist()
                    return embedding
            
            self.embeddings = CustomTransformerEmbeddings()
            print("âœ… Embedding modeli baÅŸlatÄ±ldÄ± (Custom Transformers - LOKAL)")
            
        except Exception as e:
            print(f"âŒ Embedding hatasÄ±: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"Lokal embedding modeli baÅŸlatÄ±lamadÄ±: {str(e)}")
    
    def _initialize_llm(self):
        """LLM modelini baÅŸlatÄ±r"""
        try:
            if self.model_provider == "ollama":
                # Ollama - Tamamen lokal, API key gerektirmez
                self.llm = ChatOllama(
                    model="phi3:mini",  # Phi-3 Mini 3.8B
                    temperature=0.7,
                    base_url="http://localhost:11434"  # Ollama varsayÄ±lan port
                )
                print("âœ“ Ollama - Phi-3 Mini modeli baÅŸlatÄ±ldÄ± (LOKAL)")
                
            elif self.model_provider == "groq":
                if not self.groq_api_key:
                    raise ValueError("Groq API anahtarÄ± bulunamadÄ±. Groq kullanmak iÃ§in API key gerekli.")
                
                self.llm = ChatGroq(
                    model="llama-3.2-90b-text-preview",  # En gÃ¼Ã§lÃ¼ model
                    groq_api_key=self.groq_api_key,
                    temperature=0.7,
                    max_tokens=2048
                )
                print("âœ“ Groq - Llama 3.2 90B modeli baÅŸlatÄ±ldÄ±")
            else:
                raise ValueError(f"Desteklenmeyen model saÄŸlayÄ±cÄ±sÄ±: {self.model_provider}. Sadece 'ollama' veya 'groq' destekleniyor.")
                
        except Exception as e:
            raise Exception(f"LLM baÅŸlatÄ±lamadÄ±: {str(e)}")
    
    def create_vectorstore(self, documents: List[Document]):
        """
        Belgelerden vektÃ¶r veritabanÄ± oluÅŸturur
        
        Args:
            documents: Document nesnelerinin listesi
        """
        try:
            print(f"\nğŸ“Š {len(documents)} belge vektÃ¶r veritabanÄ±na ekleniyor...")
            
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            
            # VeritabanÄ±nÄ± kaydet
            self.vectorstore.persist()
            print(f"âœ“ VektÃ¶r veritabanÄ± oluÅŸturuldu ve kaydedildi: {self.persist_directory}")
            
        except Exception as e:
            raise Exception(f"VektÃ¶r veritabanÄ± oluÅŸturulamadÄ±: {str(e)}")
    
    def load_vectorstore(self):
        """Mevcut vektÃ¶r veritabanÄ±nÄ± yÃ¼kler"""
        try:
            if not os.path.exists(self.persist_directory):
                raise FileNotFoundError(f"VektÃ¶r veritabanÄ± bulunamadÄ±: {self.persist_directory}")
            
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            print("âœ“ VektÃ¶r veritabanÄ± yÃ¼klendi")
            
        except Exception as e:
            raise Exception(f"VektÃ¶r veritabanÄ± yÃ¼klenemedi: {str(e)}")
    
    def create_qa_chain(self, k: int = 4):
        """
        Soru-cevap zinciri oluÅŸturur
        
        Args:
            k: Her sorgu iÃ§in alÄ±nacak benzer belge sayÄ±sÄ±
        """
        if self.vectorstore is None:
            raise ValueError("Ã–nce vektÃ¶r veritabanÄ± oluÅŸturulmalÄ± veya yÃ¼klenmelidir")
        
        try:
            # Retriever oluÅŸtur
            retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
            
            # QA zinciri oluÅŸtur
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                verbose=False
            )
            
            print(f"âœ“ QA zinciri oluÅŸturuldu (k={k})")
            
        except Exception as e:
            raise Exception(f"QA zinciri oluÅŸturulamadÄ±: {str(e)}")
    
    def query(self, question: str) -> dict:
        """
        KullanÄ±cÄ± sorusuna cevap Ã¼retir
        
        Args:
            question: KullanÄ±cÄ±nÄ±n sorusu
            
        Returns:
            Cevap ve kaynak belgeler iÃ§eren dict
        """
        if self.qa_chain is None:
            raise ValueError("Ã–nce QA zinciri oluÅŸturulmalÄ±dÄ±r")
        
        try:
            # Prompt'u TÃ¼rkÃ§e iÃ§in optimize et
            enhanced_question = f"""
Sen bir film uzmanÄ± asistanÄ±sÄ±n. Sana verilen film eleÅŸtirileri bilgi tabanÄ±nÄ± kullanarak 
kullanÄ±cÄ±nÄ±n sorusuna detaylÄ±, bilgilendirici ve dostÃ§a bir ÅŸekilde cevap ver.

KullanÄ±cÄ± Sorusu: {question}

LÃ¼tfen cevabÄ±nÄ± TÃ¼rkÃ§e olarak ver ve mÃ¼mkÃ¼nse bilgi tabanÄ±ndaki spesifik film Ã¶rnekleriyle destekle.
"""
            
            # Sorguyu Ã§alÄ±ÅŸtÄ±r
            result = self.qa_chain({"query": enhanced_question})
            
            return {
                'answer': result['result'],
                'source_documents': result['source_documents'],
                'question': question
            }
            
        except Exception as e:
            return {
                'answer': f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}",
                'source_documents': [],
                'question': question
            }
    
    def get_similar_documents(self, query: str, k: int = 3) -> List[Document]:
        """
        Sorguya en benzer belgeleri dÃ¶ndÃ¼rÃ¼r
        
        Args:
            query: Arama sorgusu
            k: DÃ¶ndÃ¼rÃ¼lecek belge sayÄ±sÄ±
            
        Returns:
            Benzer belgelerin listesi
        """
        if self.vectorstore is None:
            raise ValueError("Ã–nce vektÃ¶r veritabanÄ± oluÅŸturulmalÄ± veya yÃ¼klenmelidir")
        
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            raise Exception(f"Benzer belgeler alÄ±namadÄ±: {str(e)}")


def main():
    """Test fonksiyonu"""
    try:
        # RAG Pipeline oluÅŸtur
        rag = RAGPipeline()
        
        # Test sorgusu
        print("\nğŸ§ª Test Sorgusu")
        print("-" * 50)
        
        # EÄŸer veritabanÄ± varsa yÃ¼kle, yoksa uyarÄ± ver
        if os.path.exists("chroma_db"):
            rag.load_vectorstore()
            rag.create_qa_chain()
            
            test_question = "Christopher Nolan'Ä±n en iyi filmleri hangileri?"
            print(f"\nSoru: {test_question}")
            
            result = rag.query(test_question)
            print(f"\nCevap:\n{result['answer']}")
            
            print(f"\nğŸ“š KullanÄ±lan kaynak sayÄ±sÄ±: {len(result['source_documents'])}")
        else:
            print("âš ï¸  VektÃ¶r veritabanÄ± bulunamadÄ±. Ã–nce verileri iÅŸleyin.")
            
    except Exception as e:
        print(f"âŒ Hata: {str(e)}")


if __name__ == "__main__":
    main()

