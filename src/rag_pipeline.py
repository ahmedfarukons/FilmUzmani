"""
RAG Pipeline - Retrieval-Augmented Generation sistemi
"""
import os

# TensorFlow'u devre dÄ±ÅŸÄ± bÄ±rak (sadece PyTorch kullan)
os.environ['USE_TF'] = 'NO'
os.environ['USE_TORCH'] = 'YES'

from typing import List, Optional
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_ollama import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI


class RAGPipeline:
    """RAG pipeline sÄ±nÄ±fÄ± - FAISS vektÃ¶r DB ve Ã§eÅŸitli LLM'ler kullanarak soru-cevap sistemi"""
    
    def __init__(
        self, 
        persist_directory: str = "faiss_db",
        model_provider: str = "ollama",
        api_key: Optional[str] = None
    ):
        """
        RAG Pipeline'Ä± baÅŸlatÄ±r
        
        Args:
            persist_directory: FAISS veritabanÄ± dizini
            model_provider: LLM saÄŸlayÄ±cÄ±sÄ± ("ollama" veya "gemini")
            api_key: API anahtarÄ± (Gemini iÃ§in gerekli, .env'den okunabilir)
        """
        # .env dosyasÄ±nÄ± yÃ¼kle
        load_dotenv()
        
        self.persist_directory = persist_directory
        self.model_provider = model_provider.lower()
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None
        
        # Embedding modelini baÅŸlat
        self._initialize_embeddings()
        
        # LLM'i baÅŸlat
        self._initialize_llm()
    
    def _initialize_embeddings(self):
        """Embedding modelini baÅŸlatÄ±r - Custom Transformers Embeddings (sentence-transformers olmadan)"""
        try:
            # Direkt transformers kullan, sentence-transformers'a gerek yok
            from transformers import AutoTokenizer, AutoModel
            import torch
            from langchain.embeddings.base import Embeddings
            from typing import List
            
            print("ğŸ“¥ Embedding modeli yÃ¼kleniyor (lokal, transformers kullanarak)...")
            
            # Custom embedding class
            class CustomTransformerEmbeddings(Embeddings):
                def __init__(self):
                    self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
                    self.model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
                    self.model.eval()
                
                def embed_documents(self, texts: List[str]) -> List[List[float]]:
                    embeddings = []
                    for text in texts:
                        encoded = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)
                        with torch.no_grad():
                            output = self.model(**encoded)
                        # Mean pooling
                        embedding = output.last_hidden_state.mean(dim=1).squeeze().tolist()
                        embeddings.append(embedding)
                    return embeddings
                
                def embed_query(self, text: str) -> List[float]:
                    encoded = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)
                    with torch.no_grad():
                        output = self.model(**encoded)
                    embedding = output.last_hidden_state.mean(dim=1).squeeze().tolist()
                    return embedding
            
            self.embeddings = CustomTransformerEmbeddings()
            print("âœ… Embedding modeli baÅŸlatÄ±ldÄ± (Custom Transformers - LOKAL)")
            
        except Exception as e:
            print(f"âŒ Embedding hatasÄ±: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"Lokal embedding modeli baÅŸlatÄ±lamadÄ±: {str(e)}")
    
    def _initialize_llm(self):
        """LLM modelini baÅŸlatÄ±r"""
        try:
            if self.model_provider == "ollama":
                # Ollama - Tamamen lokal, API key gerektirmez
                ollama_base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
                ollama_model = os.getenv('OLLAMA_MODEL', 'phi3:mini')
                temperature = float(os.getenv('OLLAMA_TEMPERATURE', '0.5'))
                num_predict = int(os.getenv('OLLAMA_NUM_PREDICT', '512'))
                self.llm = ChatOllama(
                    model=ollama_model,
                    temperature=temperature,
                    base_url=ollama_base_url,
                    num_predict=num_predict
                )
                print(f"âœ“ Ollama modeli baÅŸlatÄ±ldÄ±: {ollama_model} ({ollama_base_url})")
            
            elif self.model_provider == "gemini":
                if not self.api_key:
                    raise ValueError("Gemini API anahtarÄ± bulunamadÄ±. .env dosyasÄ±na GEMINI_API_KEY ekleyin.")
                
                gemini_model = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
                temperature = float(os.getenv('GEMINI_TEMPERATURE', '0.7'))
                self.llm = ChatGoogleGenerativeAI(
                    model=gemini_model,
                    google_api_key=self.api_key,
                    temperature=temperature,
                    convert_system_message_to_human=True
                )
                print(f"âœ“ Google Gemini modeli baÅŸlatÄ±ldÄ±: {gemini_model} ğŸš€")
            
            else:
                raise ValueError(f"Desteklenmeyen model: {self.model_provider}. Sadece 'ollama' veya 'gemini' destekleniyor.")
                
        except Exception as e:
            raise Exception(f"LLM baÅŸlatÄ±lamadÄ±: {str(e)}")
    
    def create_vectorstore(self, documents: List[Document]):
        """
        Belgelerden vektÃ¶r veritabanÄ± oluÅŸturur
        
        Args:
            documents: Document nesnelerinin listesi
        """
        try:
            print(f"\nğŸ“Š {len(documents)} belge vektÃ¶r veritabanÄ±na ekleniyor...")
            
            # FAISS vektÃ¶r DB oluÅŸtur
            self.vectorstore = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )
            
            # FAISS'i kaydet
            self.vectorstore.save_local(self.persist_directory)
            print(f"âœ“ VektÃ¶r veritabanÄ± oluÅŸturuldu ve kaydedildi: {self.persist_directory}")
            
        except Exception as e:
            raise Exception(f"VektÃ¶r veritabanÄ± oluÅŸturulamadÄ±: {str(e)}")
    
    def load_vectorstore(self):
        """Mevcut vektÃ¶r veritabanÄ±nÄ± yÃ¼kler"""
        try:
            if not os.path.exists(self.persist_directory):
                raise FileNotFoundError(f"VektÃ¶r veritabanÄ± bulunamadÄ±: {self.persist_directory}")
            
            # FAISS'i yÃ¼kle
            self.vectorstore = FAISS.load_local(
                self.persist_directory,
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True  # Local dosya gÃ¼venli
            )
            print("âœ“ VektÃ¶r veritabanÄ± yÃ¼klendi")
            
        except Exception as e:
            raise Exception(f"VektÃ¶r veritabanÄ± yÃ¼klenemedi: {str(e)}")
    
    def create_qa_chain(self, k: int = 4):
        """
        Soru-cevap zinciri oluÅŸturur
        
        Args:
            k: Her sorgu iÃ§in alÄ±nacak benzer belge sayÄ±sÄ±
        """
        if self.vectorstore is None:
            raise ValueError("Ã–nce vektÃ¶r veritabanÄ± oluÅŸturulmalÄ± veya yÃ¼klenmelidir")
        
        try:
            # Retriever oluÅŸtur
            retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
            
            # QA zinciri oluÅŸtur
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                verbose=False
            )
            
            print(f"âœ“ QA zinciri oluÅŸturuldu (k={k}) - HIZLI MOD AKTÄ°F")
            
        except Exception as e:
            raise Exception(f"QA zinciri oluÅŸturulamadÄ±: {str(e)}")
    
    def query(self, question: str) -> dict:
        """
        KullanÄ±cÄ± sorusuna cevap Ã¼retir
        
        Args:
            question: KullanÄ±cÄ±nÄ±n sorusu
            
        Returns:
            Cevap ve kaynak belgeler iÃ§eren dict
        """
        if self.qa_chain is None:
            raise ValueError("Ã–nce QA zinciri oluÅŸturulmalÄ±dÄ±r")
        
        try:
            # Prompt'u TÃ¼rkÃ§e iÃ§in optimize et
            enhanced_question = f"""
Sen bir film uzmanÄ± asistanÄ±sÄ±n. Sana verilen film eleÅŸtirileri bilgi tabanÄ±nÄ± kullanarak 
kullanÄ±cÄ±nÄ±n sorusuna detaylÄ±, bilgilendirici ve dostÃ§a bir ÅŸekilde cevap ver.

KullanÄ±cÄ± Sorusu: {question}

LÃ¼tfen cevabÄ±nÄ± TÃ¼rkÃ§e olarak ver ve mÃ¼mkÃ¼nse bilgi tabanÄ±ndaki spesifik film Ã¶rnekleriyle destekle.
"""
            
            # Sorguyu Ã§alÄ±ÅŸtÄ±r (invoke kullan, __call__ deprecated)
            result = self.qa_chain.invoke({"query": enhanced_question})
            
            return {
                'answer': result['result'],
                'source_documents': result['source_documents'],
                'question': question
            }
            
        except Exception as e:
            return {
                'answer': f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}",
                'source_documents': [],
                'question': question
            }
    
    def get_similar_documents(self, query: str, k: int = 3) -> List[Document]:
        """
        Sorguya en benzer belgeleri dÃ¶ndÃ¼rÃ¼r
        
        Args:
            query: Arama sorgusu
            k: DÃ¶ndÃ¼rÃ¼lecek belge sayÄ±sÄ±
            
        Returns:
            Benzer belgelerin listesi
        """
        if self.vectorstore is None:
            raise ValueError("Ã–nce vektÃ¶r veritabanÄ± oluÅŸturulmalÄ± veya yÃ¼klenmelidir")
        
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            raise Exception(f"Benzer belgeler alÄ±namadÄ±: {str(e)}")


def main():
    """Test fonksiyonu"""
    try:
        # RAG Pipeline oluÅŸtur
        rag = RAGPipeline()
        
        # Test sorgusu
        print("\nğŸ§ª Test Sorgusu")
        print("-" * 50)
        
        # EÄŸer veritabanÄ± varsa yÃ¼kle, yoksa uyarÄ± ver
        if os.path.exists("faiss_db"):
            rag.load_vectorstore()
            rag.create_qa_chain()
            
            test_question = "Christopher Nolan'Ä±n en iyi filmleri hangileri?"
            print(f"\nSoru: {test_question}")
            
            result = rag.query(test_question)
            print(f"\nCevap:\n{result['answer']}")
            
            print(f"\nğŸ“š KullanÄ±lan kaynak sayÄ±sÄ±: {len(result['source_documents'])}")
        else:
            print("âš ï¸  VektÃ¶r veritabanÄ± bulunamadÄ±. Ã–nce verileri iÅŸleyin.")
            
    except Exception as e:
        print(f"âŒ Hata: {str(e)}")


if __name__ == "__main__":
    main()

